import requests
from bs4 import BeautifulSoup
import json
import os
import datetime

# --- 1. é…ç½®åŒºï¼šæƒ…æŠ¥æºæ¸…å• ---
MONITOR_SOURCES = [
    {"name": "ğŸ“š æ•™åŠ¡å¤„", "url": "http://jwc.swjtu.edu.cn/vatuu/WebAction?setAction=newsList"},
    {"name": "âš¡ ç”µæ°”æ–°é—»", "url": "https://dqxy.swjtu.edu.cn/xwdt/qb.htm"},
    {"name": "âš¡ ç”µæ°”å…¬å‘Š", "url": "https://dqxy.swjtu.edu.cn/tzgg/qb.htm"}, # å»ºè®®æŠŠå…¬å‘Šä¹Ÿä¿ç•™ï¼Œé€šå¸¸å…¬å‘Šæ¯”æ–°é—»æ›´å…³ä¹åˆ‡èº«åˆ©ç›Š
    {"name": "ğŸ“¢ å­¦æ ¡é€šçŸ¥", "url": "https://news.swjtu.edu.cn/zx/tzgg.htm"},
    {"name": "ğŸ”¬ å­¦æœ¯æ´»åŠ¨", "url": "https://news.swjtu.edu.cn/zx/xshd.htm"}
]
# äº‘ç«¯ç›¸å¯¹è·¯å¾„ï¼ˆå¯¹åº” GitHub ä»“åº“ç»“æ„ï¼‰
HISTORY_FILE = "history_titles.txt"
RESERVOIR_FILE = "pending_news.json"
POSTS_DIR = "source/_posts/"


# --- 2. çˆ¬è™«å‡½æ•°ï¼šæŠ“å–çœŸå®æ•°æ® ---
def fetch_all_titles():
    all_found = []
    # ä¼ªè£…æˆæµè§ˆå™¨
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}

    for source in MONITOR_SOURCES:
        try:
            print(f"ğŸ“¡ æ­£åœ¨æœå¯»ã€{source['name']}ã€‘...")
            # è®¾ç½® 15ç§’è¶…æ—¶ï¼Œé˜²æ­¢å­¦æ ¡æœåŠ¡å™¨å¡é¡¿
            res = requests.get(source["url"], headers=headers, timeout=15)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, "html.parser")

            # é’ˆå¯¹ä¸åŒç½‘ç«™ç»“æ„çš„é€šç”¨æå–ç­–ç•¥
            for link in soup.find_all('a'):
                t = link.get_text(strip=True)
                # è¿‡æ»¤æ‰çŸ­æ ‡é¢˜ï¼ˆå¯¼èˆªæ ã€é¡µè„šç­‰ï¼‰
                if len(t) > 12:
                    all_found.append(t)
        except Exception as e:
            print(f"âŒ {source['name']} è®¿é—®å¼‚å¸¸: {e}")

    # å»é‡åè¿”å›
    return list(set(all_found))


# --- 3. æ ¸å¿ƒï¼šè“„æ°´æ± å†³ç­–é€»è¾‘ ---
def handle_data(scraped_titles):
    # A. è¯»å–å†å²è®°å½•ï¼ˆé˜²æ­¢é‡å¤æŠ“å–ï¼‰
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            history = f.read().splitlines()
    else:
        history = []

   # B. è¯»å–è“„æ°´æ± ï¼ˆæŸ¥çœ‹ç§¯å‹æƒ…å†µï¼‰
    if os.path.exists(RESERVOIR_FILE):
        with open(RESERVOIR_FILE, "r", encoding="utf-8") as f:
            reservoir = json.load(f)
            # å…¼å®¹è€æ•°æ®ï¼šå¦‚æœä¹‹å‰çš„ json é‡Œæ²¡æœ‰ issue_numberï¼Œå°±ç»™å®ƒåŠ ä¸Šåˆå§‹å€¼ 5
            if "issue_number" not in reservoir:
                reservoir["issue_number"] = 5
    else:
        # åˆå§‹åŒ–ï¼šé»˜è®¤ä¸Šæ¬¡å‘å¸ƒæ˜¯ 5 å¤©å‰ï¼ŒæœŸæ•°ä» 5 å¼€å§‹
        five_days_ago = (datetime.date.today() - datetime.timedelta(days=5)).strftime("%Y-%m-%d")
        reservoir = {"news": [], "last_post_date": five_days_ago, "issue_number": 5}

    # C. ç­›é€‰å‡ºçœŸæ­£çš„æ–°é—»
    fresh_news = [t for t in scraped_titles if t not in history]

    # D. æ›´æ–°è“„æ°´æ± å’Œå†å²
    if fresh_news:
        print(f"âœ¨ å‘ç° {len(fresh_news)} æ¡æ–°åŠ¨æ€ï¼Œå·²å­˜å…¥è“„æ°´æ± ã€‚")
        reservoir["news"].extend(fresh_news)
        with open(HISTORY_FILE, "a", encoding="utf-8") as f:
            for t in fresh_news: f.write(t + "\n")


    today = datetime.date.today()
    try:
        last_date = datetime.datetime.strptime(reservoir["last_post_date"], "%Y-%m-%d").date()
    except:
        last_date = today - datetime.timedelta(days=10)  # å®¹é”™å¤„ç†

    days_passed = (today - last_date).days

    should_publish = (len(reservoir["news"]) >= 8) or (days_passed >= 5 and len(reservoir["news"]) > 3)

    return should_publish, reservoir


# --- 4. AI å¤§è„‘ï¼šDeepSeek æ·±åº¦åˆ†æ ---
def ask_deepseek_summary(news_list):
    # ã€å®‰å…¨å…³é”®ã€‘ä» GitHub ç¯å¢ƒå˜é‡è·å– Key
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("âŒ è‡´å‘½é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼æ— æ³•è°ƒç”¨ AIã€‚")
        return None

    url = "https://api.deepseek.com/chat/completions"
    raw_report = "\n".join([f"- {t}" for t in news_list])

    # ä½ çš„æ ¸å¿ƒ Prompt
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªè´Ÿè´£ä»»çš„æ ¡å›­æ–°é—»æ‘˜è¦åŠ©æ‰‹ï¼Œä»»åŠ¡æ˜¯ä¸ºè¥¿å—äº¤å¤§ç”µæ°”å·¥ç¨‹ä¸“ä¸šçš„å¤§ä¸€å­¦ç”Ÿç­›é€‰å’Œæç‚¼æ ¡å›­å®˜ç½‘æ–°é—»ã€‚è¯¥å­¦ç”Ÿå¯¹Pythonå¼€å‘ã€AIå¤§æ¨¡å‹ã€æ•°å­¦å»ºæ¨¡ã€åµŒå…¥å¼ã€ç”µå­è®¾è®¡ç¨å¾®æœ‰ä¸€ç‚¹æ„Ÿå…´è¶£ã€‚

    è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è§„åˆ™å¤„ç†ä»Šå¤©æŠ“å–çš„æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼š

    1. **ä¸¥æ ¼è¿‡æ»¤**ï¼šæ‰€æœ‰é¢†å¯¼å¼€ä¼šã€è§†å¯Ÿèµ°è®¿ã€å…šå»ºå­¦ä¹ ã€å¼€å­¦æ£€æŸ¥ã€èŠ‚æ—¥é—®å€™ç­‰çº¯è¡Œæ”¿å®£ä¼ ç±»é€šç¨¿ã€‚åœ¨ç»“æœä¸­ç•¥å¾®æåŠä¸€ä¸¤å¥è¿™äº›æ–°é—»å³å¯ã€‚
    2. **æå–æ ¸å¿ƒ**ï¼šé‡ç‚¹æŒ‘é€‰å‡ºäº‹å…³å­¦ç”Ÿåˆ‡èº«åˆ©ç›Šçš„â€œæ•™åŠ¡é€šçŸ¥â€ï¼ˆå¦‚é€‰è¯¾ã€è€ƒè¯•ã€å››å…­çº§ã€æ”¾å‡ï¼‰ã€â€œå­¦æœ¯ç«èµ›â€ï¼ˆé‡ç‚¹ä¿ç•™AIã€è½¯ä»¶ã€åµŒå…¥å¼ã€æ•°å­¦å»ºæ¨¡æ–¹å‘ï¼Œå‰©ä¸‹æ–¹å‘æåˆ°å³å¯ï¼‰ä»¥åŠâ€œå­¦é™¢é‡è¦åŠ¨æ€â€ï¼ˆå¦‚å¥–å­¦é‡‘ã€è¯„ä¼˜ã€é‡å¤§ç§‘ç ”çªç ´ï¼‰ã€‚
    3. **è¯­è¨€é£æ ¼**ï¼šä½¿ç”¨å®¢è§‚ã€ç®€ç»ƒã€å¹³å®çš„è¯­è¨€è¿›è¡Œæ€»ç»“ï¼Œåƒæ­£å¸¸çš„å­¦é•¿åœ¨ç¾¤é‡Œå‘é€šçŸ¥ä¸€æ ·ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•å¤¸å¼ ã€ä¸­äºŒæˆ–è¿‡åº¦æ‹ŸäººåŒ–çš„è¯æ±‡ã€‚

    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ Markdown æ ¼å¼è¾“å‡ºåˆ†ç±»ï¼ˆå¦‚æœæ²¡æœ‰æŸç±»çš„å†…å®¹ï¼Œè¯·ç›´æ¥çœç•¥è¯¥åˆ†ç±»ï¼Œå¦‚æœæœ‰è¿™äº›åˆ†ç±»æ²¡æœ‰æ¶‰åŠçš„æ–°é—» è¯·æ¨¡ä»¿ä»¥ä¸‹æ ¼å¼è‡ªè¡Œè¾“å‡ºï¼‰ï¼š

    ### ğŸš¨ ç´§æ€¥ä¸æ ¸å¿ƒé¢„è­¦ (Urgent & Important)
    > æå–å¸¦æœ‰æ˜ç¡®æ—¶é—´èŠ‚ç‚¹ï¼ˆDeadlineï¼‰æˆ–å¿…é¡»ç«‹åˆ»æ‰§è¡Œçš„æ“ä½œã€‚
    - **[æ–°é—»æ ‡é¢˜]**ï¼šæ˜ç¡®è¯´æ˜æˆªæ­¢æ—¶é—´å’Œéœ€è¦è¿›è¡Œçš„æ“ä½œï¼ˆå¦‚ï¼šé€‰è¯¾æŠ¢è¯¾ã€å››å…­çº§æŠ¥å/æŸ¥åˆ†ã€è¿”æ ¡å¡«æŠ¥ã€è¡¥è€ƒç¡®è®¤ç­‰ï¼‰ã€‚

    ### ğŸ“Œ æ•™åŠ¡ä¸åŸ¹å…»åŠ¨æ€ (Academic Affairs)
    > äº‹å…³å­¦ä¸šè§„åˆ’çš„å¸¸è§„é€šçŸ¥ã€‚
    - **[æ–°é—»æ ‡é¢˜]**ï¼šæç‚¼é€šçŸ¥é‡ç‚¹ï¼ˆå¦‚ï¼šåŸ¹å…»æ–¹æ¡ˆè°ƒæ•´ã€å…¬é€‰è¯¾åå•ã€æ”¾å‡ä¸æ ¡å†å®‰æ’ï¼‰ã€‚

    ### ğŸš€ ç«èµ›ã€è®²åº§ä¸ç§‘ç ”æ‹›å‹Ÿ (Tech & Competitions)
    > èšç„¦ä¸“ä¸šèƒ½åŠ›æ‹“å±•ã€‚
    - **[æ–°é—»æ ‡é¢˜]**ï¼šç®€è¦æ¦‚æ‹¬æ ¸å¿ƒä¸»é¢˜ã€‚é‡ç‚¹æ˜¯æ•°å­¦å»ºæ¨¡ã€è½¯ä»¶å¼€å‘ã€AIå¤§æ¨¡å‹ã€åµŒå…¥å¼ç³»ç»ŸåŠç”µæ°”å·¥ç¨‹å‰æ²¿æ–¹å‘çš„æ¯”èµ›æˆ–è®²åº§ï¼Œå…¶ä½™æåˆ°å³å¯ã€‚

    ### ğŸ’° è¯„ä¼˜ã€å¥–å­¦é‡‘ä¸æ”¿ç­– (Honors & Policies)
    > äº‹å…³ç»¼æµ‹ä¸ä¸ªäººå±¥å†ã€‚
    - **[æ–°é—»æ ‡é¢˜]**ï¼šæç‚¼æ ¸å¿ƒè¯„é€‰æ¡ä»¶æˆ–æ”¿ç­–å˜åŒ–ï¼ˆå¦‚ï¼šå¥–åŠ©å­¦é‡‘è¯„å®šã€ä¿ç ”ç»†åˆ™ã€ä¼˜ç§€ç­é›†ä½“è¯„é€‰ï¼‰ã€‚

    ### ğŸŒ æ ¡å›­ç”Ÿæ´»ä¸ITæœåŠ¡ (Campus Services)
    > åå‹¤ä¸åŸºç¡€è®¾æ–½ã€‚
    - **[æ–°é—»æ ‡é¢˜]**ï¼šæç‚¼å½±å“æ—¥å¸¸ç”Ÿæ´»çš„å®è´¨ä¿¡æ¯ï¼ˆå¦‚ï¼šæ ¡å›­ç½‘æ–­ç½‘ç»´æŠ¤ã€å›¾ä¹¦é¦†å¼€æ”¾æ—¶é—´è°ƒæ•´ã€é£Ÿå ‚/å®¿èˆé€šçŸ¥ï¼‰ã€‚
   

    """

    payload = {
        "model": "deepseek-reasoner",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ä»¥ä¸‹æ˜¯è“„æ°´æ± ä¸­çš„æœ€æ–°æƒ…æŠ¥æ±‡æ€»ï¼š\n{raw_report}"}
        ]
    }
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}

    print("\n--- ğŸ¤– æ­£åœ¨è¿æ¥ DeepSeek è¿›è¡Œå¤šç»´åº¦åˆ†æ ---")
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            print("âœ… AI å“åº”æˆåŠŸï¼")
            return content
        else:
            print(f"âŒ AI è°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        print(f"âŒ è¿æ¥ AI å‡ºé”™: {e}")
        return None


# --- [æ–°å¢] PushPlus å¾®ä¿¡æ¨é€æœåŠ¡ ---
def push_to_wechat(summary_content, blog_url="https://1nuo.me"):
    token = os.getenv("PUSHPLUS_TOKEN")

    if not token:
        print("âš ï¸ æœªæ£€æµ‹åˆ° PUSHPLUS_TOKENï¼Œè·³è¿‡æ¨é€ã€‚")
        return

    print("ğŸ“¨ æ­£åœ¨é€šè¿‡ PushPlus å‘é€å¾®ä¿¡æƒ…æŠ¥...")
    url = "http://www.pushplus.plus/send"

    # æ„é€  Markdown æ¶ˆæ¯å†…å®¹
    markdown_text = f"""
### âš¡ è¥¿å—äº¤å¤§ç”µæ°”æƒ…æŠ¥å±€
> ğŸ“… æ—¥æœŸï¼š{datetime.datetime.now().strftime('%Y-%m-%d')}
> ğŸ¤– åˆ†æå‘˜ï¼šDeepSeek 

---

{summary_content}

---
[ğŸ‘‰ ç‚¹å‡»æŸ¥çœ‹åšå®¢å®Œæ•´æ’ç‰ˆ]({blog_url})
"""

    payload = {
        "token": token,
        "title": f"âš¡ æƒ…æŠ¥å±€æ›´æ–°æé†’ ({datetime.date.today()})",
        "content": markdown_text,
        "template": "markdown"
    }

    try:
        res = requests.post(url, json=payload, timeout=10)
        resp_json = res.json()
        if resp_json['code'] == 200:
            print("âœ… PushPlus æ¨é€æˆåŠŸï¼")
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {resp_json['msg']}")
    except Exception as e:
        print(f"âŒ æ¨é€ç½‘ç»œé”™è¯¯: {e}")


# --- [ä¿®æ”¹] 5. æ‰§è¡Œä¸»æµç¨‹ ---
# æ³¨æ„ï¼šè¿™é‡Œ def å¿…é¡»é¡¶æ ¼å†™ï¼Œä¸èƒ½ç¼©è¿›ï¼
def run_satellite():
    # 1. æŠ“å–
    titles = fetch_all_titles()

    # 2. å†³ç­–
    trigger, data = handle_data(titles)

    if trigger:
        print(f"ğŸš€ è§¦å‘å‘å¸ƒæ¡ä»¶ï¼ç§¯å‹ {len(data['news'])} æ¡ï¼Œå‡†å¤‡è°ƒç”¨ AI...")

        # 3. è°ƒç”¨ AI
        ai_content = ask_deepseek_summary(data['news'])

        if ai_content:
            # --- æå–å½“å‰æœŸæ•° ---
            current_issue = data.get("issue_number", 5)
            
            # 4. ç”Ÿæˆ Hexo æ–‡ä»¶
            today_str = datetime.datetime.now().strftime("%Y-%m-%d")
            file_name = f"{POSTS_DIR}{today_str}-ee-intelligence.md"

            # ä¿®æ­£ï¼šFront Matter åŠ å…¥è‡ªåŠ¨è®¡ç®—çš„æœŸæ•°
            front_matter = f"""---
title: è¥¿å—äº¤å¤§ç”µæ°”ç®€æŠ¥ (Vol.{current_issue}) | {today_str}
date: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
tags: [AI, ç”µæ°”å·¥ç¨‹, è¥¿å—äº¤é€šå¤§å­¦]
categories: [AIç®€æŠ¥]
top_img: /img/categoriesbanner.jpg 
---

> ğŸ“¡ **æƒ…æŠ¥å‘˜æ³¨**ï¼šæœ¬ç®€æŠ¥ç”± GitHub Actions äº‘ç«¯è‡ªåŠ¨ç”Ÿæˆã€‚å½“å‰ä¸ºç¬¬ **{current_issue}** æœŸã€‚

"""
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            if not os.path.exists(POSTS_DIR):
                os.makedirs(POSTS_DIR)

            with open(file_name, "w", encoding="utf-8") as f:
                f.write(front_matter + ai_content)

            print(f"âœ¨ æ–‡ç« å·²ç”Ÿæˆï¼š{file_name} (ç¬¬ {current_issue} æœŸ)")

            # è°ƒç”¨å¾®ä¿¡æ¨é€
            push_to_wechat(ai_content, blog_url="https://1nuo.me")

            # 5. æ¸…ç©ºè“„æ°´æ±  & æ›´æ–°æ—¶é—´ & æœŸæ•°è‡ªåŠ¨ +1
            data["news"] = []
            data["last_post_date"] = today_str
            data["issue_number"] = current_issue + 1  # æ ¸å¿ƒï¼šä¸ºä¸‹ä¸€æœŸåšå‡†å¤‡
            
            with open(RESERVOIR_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        else:
            print("âš ï¸ AI æœªè¿”å›å†…å®¹ï¼Œæš‚åœå‘å¸ƒï¼Œä¿ç•™è“„æ°´æ± ã€‚")
    else:
        # æ²¡è§¦å‘ï¼Œåªä¿å­˜è“„æ°´æ± çŠ¶æ€
        with open(RESERVOIR_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"ğŸ’§ è“„æ°´æœªæ»¡ï¼Œç»§ç»­ç­‰å¾…... (å½“å‰ç§¯å‹: {len(data['news'])} æ¡)")


if __name__ == "__main__":
    run_satellite()
