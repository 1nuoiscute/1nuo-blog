import requests
from bs4 import BeautifulSoup
import json
import os
import datetime

# --- 1. é…ç½®åŒºï¼šæƒ…æŠ¥æºæ¸…å• ---
MONITOR_SOURCES = [
    {"name": "âš¡ ç”µæ°”å­¦é™¢", "url": "https://dqxy.swjtu.edu.cn/tzgg/qb.htm"},
    {"name": "ğŸ“¢ é€šçŸ¥å…¬å‘Š", "url": "https://news.swjtu.edu.cn/zx/tzgg.htm"},
    {"name": "ğŸ›ï¸ é™¢éƒ¨åŠ¨æ€", "url": "https://news.swjtu.edu.cn/zx/ybdt.htm"},
    {"name": "ğŸ“Œ äº¤å¤§è¦é—»", "url": "https://news.swjtu.edu.cn/zx/jdyw.htm"},
    {"name": "ğŸ”¬ å­¦æœ¯æ´»åŠ¨", "url": "https://news.swjtu.edu.cn/zx/xshd.htm"}
]

# äº‘ç«¯ç›¸å¯¹è·¯å¾„ï¼ˆå¯¹åº” GitHub ä»“åº“ç»“æ„ï¼‰
HISTORY_FILE = "history_titles.txt"
RESERVOIR_FILE = "pending_news.json"
POSTS_DIR = "source/_posts/"


# --- 2. çˆ¬è™«å‡½æ•°ï¼šæŠ“å–çœŸå®æ•°æ® ---
def fetch_all_titles():
    all_found = []
    # ä¼ªè£…æˆæµè§ˆå™¨
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}

    for source in MONITOR_SOURCES:
        try:
            print(f"ğŸ“¡ æ­£åœ¨æœå¯»ã€{source['name']}ã€‘...")
            # è®¾ç½® 15ç§’è¶…æ—¶ï¼Œé˜²æ­¢å­¦æ ¡æœåŠ¡å™¨å¡é¡¿
            res = requests.get(source["url"], headers=headers, timeout=15)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, "html.parser")

            # é’ˆå¯¹ä¸åŒç½‘ç«™ç»“æ„çš„é€šç”¨æå–ç­–ç•¥
            for link in soup.find_all('a'):
                t = link.get_text(strip=True)
                # è¿‡æ»¤æ‰çŸ­æ ‡é¢˜ï¼ˆå¯¼èˆªæ ã€é¡µè„šç­‰ï¼‰
                if len(t) > 12:
                    all_found.append(t)
        except Exception as e:
            print(f"âŒ {source['name']} è®¿é—®å¼‚å¸¸: {e}")

    # å»é‡åè¿”å›
    return list(set(all_found))


# --- 3. æ ¸å¿ƒï¼šè“„æ°´æ± å†³ç­–é€»è¾‘ ---
def handle_data(scraped_titles):
    # A. è¯»å–å†å²è®°å½•ï¼ˆé˜²æ­¢é‡å¤æŠ“å–ï¼‰
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            history = f.read().splitlines()
    else:
        history = []

    # B. è¯»å–è“„æ°´æ± ï¼ˆæŸ¥çœ‹ç§¯å‹æƒ…å†µï¼‰
    if os.path.exists(RESERVOIR_FILE):
        with open(RESERVOIR_FILE, "r", encoding="utf-8") as f:
            reservoir = json.load(f)
    else:
        # åˆå§‹åŒ–ï¼šé»˜è®¤ä¸Šæ¬¡å‘å¸ƒæ˜¯ 5 å¤©å‰ï¼Œç¡®ä¿é¦–æ¬¡è¿è¡Œèƒ½æ­£å¸¸åˆ¤æ–­
        five_days_ago = (datetime.date.today() - datetime.timedelta(days=5)).strftime("%Y-%m-%d")
        reservoir = {"news": [], "last_post_date": five_days_ago}

    # C. ç­›é€‰å‡ºçœŸæ­£çš„æ–°é—»
    fresh_news = [t for t in scraped_titles if t not in history]

    # D. æ›´æ–°è“„æ°´æ± å’Œå†å²
    if fresh_news:
        print(f"âœ¨ å‘ç° {len(fresh_news)} æ¡æ–°åŠ¨æ€ï¼Œå·²å­˜å…¥è“„æ°´æ± ã€‚")
        reservoir["news"].extend(fresh_news)
        with open(HISTORY_FILE, "a", encoding="utf-8") as f:
            for t in fresh_news: f.write(t + "\n")

    # E. å†³ç­–ï¼šæ˜¯å¦è§¦å‘ AI å‘å¸ƒï¼Ÿ
    # æ¡ä»¶ï¼šç§¯å‹ â‰¥ 5 æ¡ OR (è·ç¦»ä¸Šæ¬¡å‘å¸ƒ â‰¥ 3 å¤© AND æœ‰åº“å­˜)
    today = datetime.date.today()
    try:
        last_date = datetime.datetime.strptime(reservoir["last_post_date"], "%Y-%m-%d").date()
    except:
        last_date = today - datetime.timedelta(days=10)  # å®¹é”™å¤„ç†

    days_passed = (today - last_date).days

    should_publish = (len(reservoir["news"]) >= 5) or (days_passed >= 3 and len(reservoir["news"]) > 0)

    return should_publish, reservoir


# --- 4. AI å¤§è„‘ï¼šDeepSeek æ·±åº¦åˆ†æ ---
def ask_deepseek_summary(news_list):
    # ã€å®‰å…¨å…³é”®ã€‘ä» GitHub ç¯å¢ƒå˜é‡è·å– Key
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("âŒ è‡´å‘½é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼æ— æ³•è°ƒç”¨ AIã€‚")
        return None

    url = "https://api.deepseek.com/chat/completions"
    raw_report = "\n".join([f"- {t}" for t in news_list])

    # ä½ çš„æ ¸å¿ƒ Prompt
    system_prompt = """
    # Role: è¥¿å—äº¤å¤§ç”µæ°”å·¥ç¨‹ä¸“ä¸šÂ·é«˜çº§æƒ…æŠ¥åˆ†æå®˜ (Digital Sentinel)

    ## Profile
    ä½ ä¸æ˜¯æ™®é€šçš„ AI æ€»ç»“å™¨ï¼Œä½ æ˜¯ä¸“ä¸ºä¸€åã€å¯¹åµŒå…¥å¼ã€AIã€æ•°å­¦å»ºæ¨¡æ„Ÿå…´è¶£çš„æ™®é€šç”µæ°”å­¦ç”Ÿã€‘å®šåˆ¶çš„æ•°å­—å¤§è„‘ã€‚ä½ ç®€æ´ã€é«˜æ•ˆã€å……æ»¡ç†å·¥ç§‘çš„ç†æ€§ã€‚

    ## Mission
    ä½ çš„ä»»åŠ¡æ˜¯å¯¹æŠ“å–çš„æ ¡å›­åŸå§‹æƒ…æŠ¥è¿›è¡Œâ€œä¿¡å·è„±æ°´â€ä¸â€œæ·±åº¦è§£æâ€ï¼Œè¾“å‡ºä¸€ä»½æ—¢èƒ½ç›´æ¥æŒ‡å¯¼è¡ŒåŠ¨ï¼Œåˆèƒ½åœ¨å¤§å±€ä¸Šç»™å‡ºå»ºè®®çš„åšå®¢ç¨¿ä»¶ã€‚

    ## Logic & Constraints
    1. **ç”µæ°”ä¼˜å…ˆæƒ**ï¼šè‹¥ã€ç”µæ°”å·¥ç¨‹å­¦é™¢ã€‘æœ‰æ–°åŠ¨æ€ï¼Œå¿…é¡»ä½œä¸ºâ€˜å¤´æ¡â€™ï¼Œå¹¶æ·±åº¦æŒ–æ˜å…¶èƒŒåçš„ä¿ç ”ã€å¥–å­¦é‡‘æˆ–ç«èµ›æ½œå°è¯ã€‚
    2. **æ‹’ç»åºŸè¯**ï¼šä¸é‡å¤æ— æ„ä¹‰çš„ç¤¼ä»ªæ–°é—»ã€‚
    3. **æå®¢è§†è§’**ï¼šå¯¹äºç§‘ç ”åŠ¨æ€ï¼Œè¦èƒ½è”æƒ³åˆ°å…·ä½“çš„åº•å±‚æŠ€æœ¯ï¼ˆå¦‚ï¼šæåˆ°æ–°èƒ½æºï¼Œè¦è”æƒ³åˆ° BMSã€PWM é€†å˜ã€æˆ–è€…ç”µåŠ›ç”µå­å˜æ¢å™¨ï¼‰ã€‚
    4. **è¡ŒåŠ¨å¯¼å‘**ï¼šæ¯ä¸€æ¿å—æœ«å°¾å¿…é¡»æœ‰ä¸€å¥ç»™ç”¨æˆ·çš„â€œAction Tipâ€ï¼ˆè¡ŒåŠ¨å»ºè®®ï¼‰ã€‚

    ## Output Format (Markdown)
    è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼ˆ>åé¢çš„å†…å®¹è¯·ä¸è¦è¾“å‡ºï¼‰ï¼š

    ---
    ### âš¡ï¸ ç”µæ°”é¢‘é“ | Signal-to-Noise: High
    > èšç„¦æœ¬é™¢æœ€ç¡¬æ ¸çš„åŠ¨æ€ã€‚
    * **[äº‹ä»¶åç§°]**ï¼šç®€è¿°ã€‚
    * * **[æ·±åº¦æ‹†è§£]**ï¼šè¯¥åŠ¨æ€å¯¹ä¿ç ”ã€ç»¼æµ‹æˆ–æŠ€æœ¯ç§¯ç´¯çš„çœŸå®ä»·å€¼ã€‚
    * **ğŸ’¡ Action Tip**ï¼š[å…·ä½“æ€ä¹ˆåš]ã€‚

    ### ğŸŒ è·¨ç•Œå“¨æ‰€ | Cross-domain Insights
    > æ‰«æå…¨æ ¡èŒƒå›´å†…å¯èƒ½ä¸â€œç”µæ°”+AI/å»ºæ¨¡â€äº§ç”Ÿè€¦åˆçš„æœºä¼šã€‚
    * **[äº‹ä»¶/åŠ¨æ€]**ï¼šè§£è¯»å…¶è·¨å­¦ç§‘ä»·å€¼ã€‚
    * **ğŸ’¡ Action Tip**ï¼š[å»ºè®®å°è¯•çš„æ–¹å‘]ã€‚

    ### ğŸ›¡ï¸ å±€é•¿ç¢ç¢å¿µ | Mental Firewall
    > é’ˆå¯¹å½“å‰åŠ¨æ€ï¼Œç»™ç”¨æˆ·ä¸€å¥å…³äºâ€œå¯¹æŠ—ç„¦è™‘â€æˆ–â€œæ˜ç¡®ç›®æ ‡â€çš„ç¡¬æ ¸å¯„è¯­ï¼ˆç”¨ç†å·¥ç§‘ç±»æ¯”ï¼‰ã€‚

    ### ğŸ« æ ¡å›­æ–°é—» | Campus News
    > è‡ªç”±å‘æŒ¥ã€‚
    * **ğŸ’¡ Action Tip**ï¼š[å»ºè®®]ã€‚
    ---
    """

    payload = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ä»¥ä¸‹æ˜¯è“„æ°´æ± ä¸­çš„æœ€æ–°æƒ…æŠ¥æ±‡æ€»ï¼š\n{raw_report}"}
        ]
    }
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}

    print("\n--- ğŸ¤– æ­£åœ¨è¿æ¥ DeepSeek è¿›è¡Œå¤šç»´åº¦åˆ†æ ---")
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            print("âœ… AI å“åº”æˆåŠŸï¼")
            return content
        else:
            print(f"âŒ AI è°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        print(f"âŒ è¿æ¥ AI å‡ºé”™: {e}")
        return None


# --- 5. æ‰§è¡Œä¸»æµç¨‹ ---
def run_satellite():
    # 1. æŠ“å–
    titles = fetch_all_titles()

    # 2. å†³ç­–
    trigger, data = handle_data(titles)

    if trigger:
        print(f"ğŸš€ è§¦å‘å‘å¸ƒæ¡ä»¶ï¼ç§¯å‹ {len(data['news'])} æ¡ï¼Œå‡†å¤‡è°ƒç”¨ AI...")

        # 3. è°ƒç”¨ AI
        ai_content = ask_deepseek_summary(data['news'])

        if ai_content:
            # 4. ç”Ÿæˆ Hexo æ–‡ä»¶
            today_str = datetime.datetime.now().strftime("%Y-%m-%d")
            file_name = f"{POSTS_DIR}{today_str}-ee-intelligence.md"

            front_matter = f"""---
title: è¥¿å—äº¤å¤§ç”µæ°”ç®€æŠ¥ | {today_str}
date: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
tags: [AI, ç”µæ°”å·¥ç¨‹, è¥¿å—äº¤é€šå¤§å­¦]
categories: [AIç®€æŠ¥]
top_img: /img/categoriesbanner.jpg 
---

> ğŸ“¡ **æƒ…æŠ¥å‘˜æ³¨**ï¼šæœ¬ç®€æŠ¥ç”± GitHub Actions äº‘ç«¯è‡ªåŠ¨ç”Ÿæˆã€‚

"""
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            if not os.path.exists(POSTS_DIR):
                os.makedirs(POSTS_DIR)

            with open(file_name, "w", encoding="utf-8") as f:
                f.write(front_matter + ai_content)

            print(f"âœ¨ æ–‡ç« å·²ç”Ÿæˆï¼š{file_name}")

            # 5. æ¸…ç©ºè“„æ°´æ±  & æ›´æ–°æ—¶é—´
            data["news"] = []
            data["last_post_date"] = today_str
            with open(RESERVOIR_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        else:
            print("âš ï¸ AI æœªè¿”å›å†…å®¹ï¼Œæš‚åœå‘å¸ƒï¼Œä¿ç•™è“„æ°´æ± ã€‚")
    else:
        # æ²¡è§¦å‘ï¼Œåªä¿å­˜è“„æ°´æ± çŠ¶æ€ï¼ˆä¸»è¦æ˜¯ä¿å­˜æ–°æŠ“åˆ°çš„æ–°é—»ï¼‰
        with open(RESERVOIR_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"ğŸ’§ è“„æ°´æœªæ»¡ï¼Œç»§ç»­ç­‰å¾…... (å½“å‰ç§¯å‹: {len(data['news'])} æ¡)")


if __name__ == "__main__":
    run_satellite()